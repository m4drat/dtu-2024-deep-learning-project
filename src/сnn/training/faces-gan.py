# -*- coding: utf-8 -*-
"""faces.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YydlQjSTQ6-QaWWryD376UFQc2_JHP_e
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns
import kagglehub
import os
import pandas as pd



from PIL import Image
from torch.utils.data import Dataset, DataLoader
from torchvision import datasets, transforms
from sklearn import metrics
from torchvision.utils import make_grid

import kagglehub

# Download latest version
path = kagglehub.dataset_download("xhlulu/140k-real-and-fake-faces")

print("Path to dataset files:", path)

classes = {1: 'real', 0: 'fake'}

class ImageDataset(Dataset):
    def __init__(self, csv_file, transform=None):
        self.data = pd.read_csv(csv_file)
        self.transform = transform
        self._labels = {v: k for k, v in classes.items()}

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        i_path = os.path.join(path, 'real_vs_fake')
        i_path = os.path.join(i_path, 'real-vs-fake')

        img_path = os.path.join(i_path, self.data.iloc[idx, 5]) # column 5 corresponds to the path
        label = self._labels[self.data.iloc[idx, 4]] # column 4 corresponds to the label (fake or real)


        image = Image.open(img_path).convert("RGB")

        if self.transform:
            image = self.transform(image)

        return image, label

HEIGHT, WIDTH = 128, 128

transform = transforms.Compose([
    transforms.Resize((HEIGHT, WIDTH)),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
])


train_dataset = ImageDataset(os.path.join(path, 'train.csv'), transform=transform)
test_dataset = ImageDataset(os.path.join(path, 'test.csv'), transform=transform)
valid_dataset = ImageDataset(os.path.join(path, 'valid.csv'), transform=transform)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)
valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=16, shuffle=False)


def accuracy(target, pred):
    return metrics.accuracy_score(target.detach().cpu().numpy(), pred.detach().cpu().numpy())

kernel_size = 5
im_size = HEIGHT

class CNNClassifier(nn.Module):
    def __init__(self, num_of_classes):
        super().__init__()

        # 1st convolution layer
        self.conv1 = nn.Conv2d(3, 64, kernel_size, padding=1)
        self.bnorm1 = nn.BatchNorm2d(64)
        conv1_size = int(self._get_cv_output_size(im_size, padding = 1)/2)


        # 2nd convolution layer
        self.conv2 = nn.Conv2d(64, 128, kernel_size)
        self.bnorm2 = nn.BatchNorm2d(128)
        conv2_size = int(self._get_cv_output_size(conv1_size)/2)


        # 3d convolution layer
        self.conv3 = nn.Conv2d(128, 256, kernel_size)
        self.bnorm3 = nn.BatchNorm2d(256)
        conv3_size = int(self._get_cv_output_size(conv2_size)/2)

        # linear decision layers
        self.fc1 = nn.Linear(256 * (conv3_size ** 2), 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, num_of_classes)

    def forward(self, x):

        x = F.max_pool2d(self.conv1(x), 2)
        x = F.leaky_relu(self.bnorm1(x))

        x = F.max_pool2d(self.conv2(x), 2)
        x = F.leaky_relu(self.bnorm2(x))

        x = F.max_pool2d(self.conv3(x), 2)
        x = F.leaky_relu(self.bnorm3(x))


        x = x.view(-1, int(x.shape.numel() / x.shape[0]))

        x = F.leaky_relu(self.fc1(x))
        x = F.dropout(x, p=0.5, training=self.training)
        x = F.leaky_relu(self.fc2(x))
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.fc3(x)

        return x

    def _get_cv_output_size(self, input, kernel = kernel_size, stride = 1, padding = 0):
        return int(np.floor((input + 2 * padding - kernel)/stride) + 1)


# Instantiate the model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = CNNClassifier(2).to(device)


optimizer = optim.Adam(model.parameters(), lr=0.002)
loss_fn = nn.CrossEntropyLoss()

num_epochs = 10
validation_every_steps = 500

step = 0

train_accuracies = []
valid_accuracies = []

for epoch in range(num_epochs):

    model.train()

    train_accuracies_batches = []

    for inputs, targets in train_loader:
        inputs, targets = inputs.to(device), targets.to(device)

        # forward pass
        output = model(inputs)
        loss = loss_fn(output, targets)

        # backdrop
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        step += 1



        train_accuracies_batches.append( torch.mean((torch.argmax(output,axis=1) == targets).float()).item() )

    train_accuracies.append(100*np.mean(train_accuracies_batches))
    print(f'Avg training accuracy for {epoch}: {100*np.mean(train_accuracies_batches)}')

    model.eval() # switch to test mode
    valid_batch_acc  = []

    for X,y in valid_loader:

      # push data to GPU
      X = X.to(device)
      y = y.to(device)

      # forward pass and loss
      with torch.no_grad():
        yHat = model(X)
        loss = loss_fn(yHat,y)

      # loss and accuracy from this batch
      valid_batch_acc.append( torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )
    # end of batch loop...

    # and get average losses and accuracies across the batches
    valid_accuracies.append(100*np.mean(valid_batch_acc))
    print(f'Avg valid accuracy for {epoch}: {100*np.mean(valid_batch_acc)}')



print("Finished training.")

torch.save(model.state_dict(), "faces-weights-gan.pth")
