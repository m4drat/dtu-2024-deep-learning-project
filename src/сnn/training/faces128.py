# -*- coding: utf-8 -*-
"""faces(1)(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lYSVxP496RbI0VL3qwSYiwwOZ-hujbtJ
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns
import kagglehub
import os
import pandas as pd
import kagglehub



from PIL import Image
from torch.utils.data import Dataset, DataLoader
from torchvision import datasets, transforms
from sklearn import metrics
from torchvision.utils import make_grid

"""### Images preprocessing"""

HEIGHT, WIDTH = 128, 128

transform = transforms.Compose([
    transforms.Resize((HEIGHT, WIDTH)),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
])

"""### Load dataset"""

# Download latest version of the dataset
path = kagglehub.dataset_download("manjilkarki/deepfake-and-real-images")


train_dataset = datasets.ImageFolder(root=os.path.join(path, 'Dataset', 'Train'), transform=transform)
test_dataset = datasets.ImageFolder(root=os.path.join(path, 'Dataset', 'Test'), transform=transform)
valid_dataset = datasets.ImageFolder(root=os.path.join(path, 'Dataset', 'Validation'), transform=transform)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)
valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=16, shuffle=False)

classes = {index: name for name, index in train_dataset.class_to_idx.items()}


"""### CNN Model"""

kernel_size = 5
im_size = HEIGHT


class CNNClassifier(nn.Module):
    def __init__(self, num_of_classes):
        super().__init__()

        # 1st convolution layer
        self.conv1 = nn.Conv2d(3, 64, kernel_size, padding=1)
        self.bnorm1 = nn.BatchNorm2d(64)
        conv1_size = int(self._get_cv_output_size(im_size, padding = 1)/2)


        # 2nd convolution layer
        self.conv2 = nn.Conv2d(64, 128, kernel_size)
        self.bnorm2 = nn.BatchNorm2d(128)
        conv2_size = int(self._get_cv_output_size(conv1_size)/2)


        # 3d convolution layer
        self.conv3 = nn.Conv2d(128, 256, kernel_size)
        self.bnorm3 = nn.BatchNorm2d(256)
        conv3_size = int(self._get_cv_output_size(conv2_size)/2)

        # linear decision layers
        self.fc1 = nn.Linear(256 * (conv3_size ** 2), 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, num_of_classes)

    def forward(self, x):

        x = F.max_pool2d(self.conv1(x), 2)
        x = F.leaky_relu(self.bnorm1(x))

        x = F.max_pool2d(self.conv2(x), 2)
        x = F.leaky_relu(self.bnorm2(x))

        x = F.max_pool2d(self.conv3(x), 2)
        x = F.leaky_relu(self.bnorm3(x))


        x = x.view(-1, int(x.shape.numel() / x.shape[0]))

        x = F.leaky_relu(self.fc1(x))
        x = F.dropout(x, p=0.5, training=self.training)
        x = F.leaky_relu(self.fc2(x))
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.fc3(x)

        return x

    def _get_cv_output_size(self, input, kernel = kernel_size, stride = 1, padding = 0):
        return int(np.floor((input + 2 * padding - kernel)/stride) + 1)

# Instantiate the model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = CNNClassifier(2).to(device)
optimizer = optim.Adam(model.parameters(), lr=0.002)
loss_fn = nn.CrossEntropyLoss()

"""### Training loop"""

num_epochs = 10
validation_every_steps = 500

step = 0
model.train()

train_accuracies = []
valid_accuracies = []


for epoch in range(num_epochs):

    train_accuracies_batches = []

    for inputs, targets in train_loader:
        inputs, targets = inputs.to(device), targets.to(device)


        optimizer.zero_grad()
        output = model(inputs)

        loss = loss_fn(output, targets)
        loss.backward()
        optimizer.step()

        # Increment step counter
        step += 1

        # Compute accuracy.
        train_accuracies_batches.append( torch.mean((torch.argmax(output,axis=1) == targets).float()).item() )


        # train_accuracies_batches.append( torch.mean((torch.argmax(output) == targets).float()).item() )

        if step % validation_every_steps == 0:

            # Append average training accuracy to list.
            train_accuracies.append(100*np.mean(train_accuracies_batches))


            train_accuracies_batches = []

            # Compute accuracies on validation set.
            valid_accuracies_batches = []
            with torch.no_grad():
                model.eval()
                for inputs, targets in valid_loader:
                    inputs, targets = inputs.to(device), targets.to(device)

                    output = model(inputs)
                    loss = loss_fn(output, targets)

                    valid_accuracies_batches.append( torch.mean((torch.argmax(output,axis=1) == targets).float()).item() )



                model.train()

            valid_accuracies.append(100*np.mean(valid_accuracies_batches))

            print(f"Step {step:<5}   training accuracy: {train_accuracies[-1]}")
            print(f"             valid accuracy: {valid_accuracies[-1]}")



print("Finished training.")

torch.save(model.state_dict(), "faces_weights_128.pth")
    
    
